{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fetal_health = pd.read_csv('fetal_health.csv')\n",
    "\n",
    "fetal_health['histogram_tendency'] = fetal_health['histogram_tendency'].astype(str)\n",
    "fetal_health = pd.get_dummies(fetal_health)\n",
    "fetal_health.fetal_health = fetal_health.fetal_health.astype(int).astype(str) #make outcome categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "fetal_health.shape\n",
    "\n",
    "fetal_health_train, fetal_health_test = train_test_split(fetal_health, \n",
    "                                    test_size = .3) \n",
    "\n",
    "\n",
    "#We have 2126 observations, so .3 should give us some of each class in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164, 24)\n",
      "(202, 24)\n",
      "(122, 24)\n"
     ]
    }
   ],
   "source": [
    "## Evaluate class imbalance\n",
    "\n",
    "fetal_health_train_1 = fetal_health_train.loc[fetal_health_train.fetal_health == '1']\n",
    "fetal_health_train_2 = fetal_health_train.loc[fetal_health_train.fetal_health == '2']\n",
    "fetal_health_train_3 = fetal_health_train.loc[fetal_health_train.fetal_health == '3']\n",
    "\n",
    "print(fetal_health_train_1.shape)\n",
    "print(fetal_health_train_2.shape)\n",
    "print(fetal_health_train_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rebalance classes in the train set\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "fetal_health_train_3 = resample(fetal_health_train_3, \n",
    "                               n_samples = fetal_health_train_1.shape[0])\n",
    "\n",
    "fetal_health_train_2 = resample(fetal_health_train_2, \n",
    "                               n_samples = fetal_health_train_1.shape[0])\n",
    "\n",
    "fetal_health_train = pd.concat([fetal_health_train_1,fetal_health_train_2,fetal_health_train_3], \n",
    "                               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "RandomForest_100 = RandomForestClassifier(\n",
    "n_estimators = 100, #I want to experiment with different values of this\n",
    "criterion = \"gini\" #I don't think this should perform too differently from entropy, but I want to try \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_100_fit = RandomForest_100.fit(X = fetal_health_train.drop('fetal_health', axis = 1), \n",
    "                  y = fetal_health_train.fetal_health)\n",
    "#In the future I might do cross validation here\n",
    "\n",
    "\n",
    "rf_100_preds = rf_100_fit.predict(X= fetal_health_test.drop('fetal_health', axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy:\n",
      "0.9373040752351097\n",
      "\n",
      "Accuracy in class 1:\n",
      "0.9755600814663951\n",
      "\n",
      "Accuracy in class 2:\n",
      "0.7419354838709677\n",
      "\n",
      "Accuracy in class 3:\n",
      "0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "## Overall accuracy\n",
    "import numpy as np\n",
    "\n",
    "agreement = (rf_100_preds == fetal_health_test.fetal_health)\n",
    "\n",
    "print(\"Overall Accuracy:\")\n",
    "\n",
    "print(np.mean(agreement))\n",
    "\n",
    "for i in range(3):\n",
    "    my_class = str(i + 1)\n",
    "    print('')\n",
    "    print(\"Accuracy in class \" + my_class + \":\")\n",
    "    print(np.mean(agreement[fetal_health_test.fetal_health == my_class]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[479,  11,   1],\n",
       "       [ 21,  69,   3],\n",
       "       [  1,   3,  50]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(fetal_health_test.fetal_health, rf_100_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for training and accuracy without rebalancing.\n",
    "# I want to do a simulation study to see if rebalancing is worth it.\n",
    "\n",
    "def no_rebalancing():\n",
    "    fetal_health_train, fetal_health_test = train_test_split(fetal_health, \n",
    "                                    test_size = .3) \n",
    "    RandomForest_100 = RandomForestClassifier(n_estimators = 100)\n",
    "    rf_100_fit = RandomForest_100.fit(X = fetal_health_train.drop('fetal_health', axis = 1), y = fetal_health_train.fetal_health)\n",
    "    rf_100_preds = rf_100_fit.predict(X= fetal_health_test.drop('fetal_health', axis = 1))\n",
    "    agreement = (rf_100_preds == fetal_health_test.fetal_health)\n",
    "    return([np.mean(agreement), \n",
    "           np.mean(agreement[fetal_health_test.fetal_health == '1']),\n",
    "           np.mean(agreement[fetal_health_test.fetal_health == '2']),\n",
    "           np.mean(agreement[fetal_health_test.fetal_health == '3'])])\n",
    "    \n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_rebalancing():\n",
    "    fetal_health_train, fetal_health_test = train_test_split(fetal_health, \n",
    "                                    test_size = .3) \n",
    "    fetal_health_train_1 = fetal_health_train.loc[fetal_health_train.fetal_health == '1']\n",
    "    fetal_health_train_2 = fetal_health_train.loc[fetal_health_train.fetal_health == '2']\n",
    "    fetal_health_train_3 = fetal_health_train.loc[fetal_health_train.fetal_health == '3']\n",
    "    \n",
    "    fetal_health_train_3 = resample(fetal_health_train_3, \n",
    "                               n_samples = fetal_health_train_1.shape[0])\n",
    "\n",
    "    fetal_health_train_2 = resample(fetal_health_train_2, \n",
    "                               n_samples = fetal_health_train_1.shape[0])\n",
    "    fetal_health_train = pd.concat([fetal_health_train_1,fetal_health_train_2,fetal_health_train_3], \n",
    "                               ignore_index=True)\n",
    "    \n",
    "    \n",
    "    RandomForest_100 = RandomForestClassifier(n_estimators = 100)\n",
    "    rf_100_fit = RandomForest_100.fit(X = fetal_health_train.drop('fetal_health', axis = 1), y = fetal_health_train.fetal_health)\n",
    "    rf_100_preds = rf_100_fit.predict(X= fetal_health_test.drop('fetal_health', axis = 1))\n",
    "    agreement = (rf_100_preds == fetal_health_test.fetal_health)\n",
    "    return([np.mean(agreement), \n",
    "           np.mean(agreement[fetal_health_test.fetal_health == '1']),\n",
    "           np.mean(agreement[fetal_health_test.fetal_health == '2']),\n",
    "           np.mean(agreement[fetal_health_test.fetal_health == '3'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_rebalancing_mat = np.matrix([\"overall\", \"1\", \"2\", '3'])\n",
    "\n",
    "no_rebalancing_mat = np.matrix([\"overall\", \"1\", \"2\", '3'])\n",
    "\n",
    "for i in range(100):\n",
    "    with_rebalancing_mat = np.vstack([with_rebalancing_mat, with_rebalancing()])\n",
    "    no_rebalancing_mat = np.vstack([no_rebalancing_mat, no_rebalancing()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_rebalancing_df = pd.DataFrame(with_rebalancing_mat[1:,:])\n",
    "no_rebalancing_df = pd.DataFrame(no_rebalancing_mat[1:,:])\n",
    "\n",
    "mat_cols = [\"overall\", \"1\", \"2\", '3']\n",
    "\n",
    "with_rebalancing_df.columns = mat_cols\n",
    "no_rebalancing_df.columns = mat_cols\n",
    "\n",
    "for col in mat_cols:\n",
    "    with_rebalancing_df[col] = with_rebalancing_df[col].astype('float')\n",
    "    no_rebalancing_df[col] = no_rebalancing_df[col].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accracy without rebalancing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overall    0.939248\n",
       "1          0.984403\n",
       "2          0.728308\n",
       "3          0.867041\n",
       "dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accracy without rebalancing\")\n",
    "no_rebalancing_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after rebalancing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overall    0.940282\n",
       "1          0.971081\n",
       "2          0.790440\n",
       "3          0.896337\n",
       "dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy after rebalancing\")\n",
    "with_rebalancing_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like oversampling was a good idea. We had a bit better accuracy on the minority class, and a slight increase in overall accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
